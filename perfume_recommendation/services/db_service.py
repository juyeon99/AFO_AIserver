import logging
import json, os
import pymysql
import random
from openai import OpenAI
from typing import List, Dict, Optional
from pathlib import Path
from datetime import datetime, timedelta
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from collections import defaultdict
from langchain_openai import ChatOpenAI
from perfume_recommendation.models.base_model import Base, Product, Note, Spice, ProductImage, Similar, SimilarText, SimilarImage

logger = logging.getLogger(__name__)

database_url = os.getenv("DATABASE_URL")
pool_recycle_prot = int(os.getenv("POOL_RECYCLE"))

# SQLAlchemy ì„¤ì •
DATABASE_URL = database_url
engine = create_engine(DATABASE_URL, pool_recycle=pool_recycle_prot)
SessionLocal = sessionmaker(bind=engine, autoflush=False, autocommit=False)

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

class DBService:
    def __init__(
        self, db_config: Dict[str, str], cache_path_prefix: str = "cache"
    ):
        self.db_config = db_config
        self.connection = self.connect_to_db()
        self.cache_path_prefix = Path(cache_path_prefix)
        self.cache_path_prefix.mkdir(exist_ok=True)
        self.cache_expiration = timedelta(days=1)  # ìºì‹± ë§Œë£Œ ì‹œê°„ (1ì¼)
        self.session = SessionLocal()
        self.gpt_client = self.initialize_gpt_client()

    def __del__(self):
        if hasattr(self, 'session'):
            self.session.close()

    def connect_to_db(self):
        try:
            connection = pymysql.connect(
                host=self.db_config["host"],
                port=int(self.db_config["port"]),
                user=self.db_config["user"],
                password=self.db_config["password"],
                database=self.db_config["database"],
                charset="utf8mb4",
                cursorclass=pymysql.cursors.DictCursor,
            )
            logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ!")
            return connection
        except pymysql.MySQLError as e:
            logger.error(f"ğŸš¨ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜: {e}")
            return None

    def initialize_gpt_client(self):
        api_key = os.getenv("OPENAI_API_KEY")
        api_base = os.getenv("OPENAI_HOST")
        return ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.7,
            openai_api_key=api_key,
            openai_api_base=api_base
        )
    
    def fetch_brands(self) -> List[str]:
        """DBì—ì„œ ë¸Œëœë“œ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        query = "SELECT DISTINCT brand FROM product;"
        try:
            with self.connection.cursor() as cursor:
                cursor.execute(query)
                brands = [row["brand"] for row in cursor.fetchall()]
            
            logger.info(f"âœ… ì´ {len(brands)}ê°œì˜ ë¸Œëœë“œ ì¡°íšŒ ì™„ë£Œ")
            return brands
        except pymysql.MySQLError as e:
            logger.error(f"ğŸš¨ ë¸Œëœë“œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def fetch_spices_by_line(self, line_id: int) -> List[Dict]:
        """íŠ¹ì • ê³„ì—´(line_id)ì— ì†í•˜ëŠ” í–¥ë£Œ(spice) ëª©ë¡ ì¡°íšŒ"""
        try:
            query = """
                SELECT id, name_kr 
                FROM spice 
                WHERE line_id = %s;
            """
            
            with self.connection.cursor() as cursor:
                cursor.execute(query, (line_id,))
                spices = cursor.fetchall()
            
            if not spices:
                logger.warning(f"âš ï¸ í•´ë‹¹ ê³„ì—´ ID({line_id})ì— ì†í•˜ëŠ” í–¥ë£Œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []

            logger.info(f"âœ… ê³„ì—´ ID({line_id})ì— í•´ë‹¹í•˜ëŠ” í–¥ë£Œ {len(spices)}ê°œ ì¡°íšŒ ì™„ë£Œ")
            return spices

        except pymysql.MySQLError as e:
            logger.error(f"ğŸš¨ í–¥ë£Œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []

    def fetch_line_data(self) -> List[Dict]:
        """
        line í…Œì´ë¸”ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ì—¬ ë°˜í™˜.

        Returns:
            List[Dict]: line í…Œì´ë¸”ì˜ ë°ì´í„°ë¥¼ í¬í•¨í•œ ë¦¬ìŠ¤íŠ¸
        """
        query = "SELECT * FROM line;"
        try:
            with self.connection.cursor() as cursor:
                cursor.execute(query)
                lines = cursor.fetchall()

            logger.info(f"âœ… line í…Œì´ë¸” ë°ì´í„° {len(lines)}ê°œ ì¡°íšŒ ì™„ë£Œ")
            return lines
        except pymysql.MySQLError as e:
            logger.error(f"ğŸš¨ ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []
    
    def get_perfumes_by_middle_notes(self, spice_ids: List[int]) -> List[Dict]:
        """MIDDLE íƒ€ì…ì˜ ë…¸íŠ¸ë¥¼ í¬í•¨í•œ í–¥ìˆ˜ë¥¼ ê²€ìƒ‰"""
        try:
            spice_ids_str = ",".join(map(str, spice_ids))
            query = f"""
                SELECT DISTINCT
                    p.id, 
                    p.brand, 
                    p.name_kr, 
                    p.size_option as volume,
                    COUNT(DISTINCT n.spice_id) as matching_count
                FROM product p
                JOIN note n ON p.id = n.product_id
                WHERE p.category_id = 1
                AND n.spice_id IN ({spice_ids_str})
                AND n.note_type = 'MIDDLE'
                GROUP BY p.id, p.brand, p.name_kr, p.size_option
                ORDER BY matching_count DESC;
            """

            with self.connection.cursor() as cursor:
                cursor.execute(query)
                perfumes = cursor.fetchall()
                logger.info(f"âœ… ì „ì²´ ë§¤ì¹­ë˜ëŠ” í–¥ìˆ˜ {len(perfumes)}ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

                return perfumes

        except pymysql.MySQLError as e:
            logger.error(f"ğŸš¨ í–¥ìˆ˜ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    # def cache_perfume_data(self, force: bool = False) -> None:
    #     """
    #     DBì˜ í–¥ìˆ˜ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ìºì‹±. `force=True` ë˜ëŠ” ë³€ê²½ ì‚¬í•­ì´ ìˆì„ ê²½ìš° ê°±ì‹ .
    #     """
    #     existing_products = self.load_cached_perfume_data(check_only=True)

    #     query = """
    #     SELECT 
    #         p.id, p.name_kr, p.name_en, p.brand, p.main_accord, p.category_id
    #     FROM product p
    #     """
    #     try:
    #         with self.connection.cursor() as cursor:
    #             cursor.execute(query)
    #             new_products = cursor.fetchall()

    #         # ë°ì´í„° ë³€ê²½ ì—¬ë¶€ í™•ì¸
    #         if not force and self.is_cache_up_to_date(existing_products, new_products):
    #             logger.info(f"âœ… ìºì‹± ë°ì´í„°ê°€ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤: {self.cache_path}")
    #             return

    #         # ìºì‹± íŒŒì¼ ì €ì¥
    #         with open(self.cache_path, "w", encoding="utf-8") as f:
    #             json.dump(new_products, f, ensure_ascii=False, indent=4)

    #         logger.info(f"âœ… í–¥ìˆ˜ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ìºì‹± ì™„ë£Œ: {self.cache_path}")

    #     except pymysql.MySQLError as e:
    #         logger.error(f"ğŸš¨ ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def cache_data(self, query: str, cache_file: Path, key_field: str, force: bool = False) -> None:
        """
        DB ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ìºì‹±. `force=True` ë˜ëŠ” ë³€ê²½ ì‚¬í•­ì´ ìˆì„ ê²½ìš° ê°±ì‹ .
        """
        existing_data = self.load_cached_data(cache_file, check_only=True)

        try:
            with self.connection.cursor() as cursor:
                cursor.execute(query)
                new_data = cursor.fetchall()

            # ë°ì´í„° ë³€ê²½ ì—¬ë¶€ í™•ì¸
            if not force and self.is_cache_up_to_date(existing_data, new_data):
                logger.info(f"âœ… ìºì‹± ë°ì´í„°ê°€ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤: {cache_file}")
                return

            # ìºì‹± íŒŒì¼ ì €ì¥
            with open(cache_file, "w", encoding="utf-8") as f:
                json.dump(new_data, f, ensure_ascii=False, indent=4)

            logger.info(f"âœ… ë°ì´í„° ìºì‹± ì™„ë£Œ: {cache_file}")

        except pymysql.MySQLError as e:
            logger.error(f"ğŸš¨ ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # def load_cached_perfume_data(self, check_only: bool = False) -> List[Dict]:
    #     """
    #     ìºì‹±ëœ ë°ì´í„°ë¥¼ ë¡œë“œ. ìºì‹± íŒŒì¼ì´ ì—†ìœ¼ë©´ check_only=Falseì¼ ë•Œ ìƒˆë¡œ ìƒì„±.
    #     """
    #     if not self.cache_path.exists():
    #         if check_only:
    #             return []
    #         logger.info("ìºì‹± íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì•„ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
    #         self.cache_perfume_data()

    #     with open(self.cache_path, "r", encoding="utf-8") as f:
    #         products = json.load(f)

    #     logger.info(f"âœ… ìºì‹±ëœ í–¥ìˆ˜ ë°ì´í„° {len(products)}ê°œ ë¡œë“œ")
    #     return products
    
    def load_cached_data(self, cache_file: Path, check_only: bool = False) -> List[Dict]:
        """
        ìºì‹±ëœ ë°ì´í„°ë¥¼ ë¡œë“œ. ìºì‹± íŒŒì¼ì´ ì—†ìœ¼ë©´ check_only=Falseì¼ ë•Œ ìƒˆë¡œ ìƒì„±.
        """
        if not cache_file.exists():
            if check_only:
                return []
            logger.info(f"ìºì‹± íŒŒì¼ {cache_file}ì´(ê°€) ì¡´ì¬í•˜ì§€ ì•Šì•„ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
            if "perfume_cache" in str(cache_file):
                self.cache_perfume_data()
            elif "diffuser_cache" in str(cache_file):
                self.cache_diffuser_data()
            elif "spice_cache" in str(cache_file):
                self.cache_spice_data()
            elif "note_cache" in str(cache_file):
                self.cache_note_data()

        with open(cache_file, "r", encoding="utf-8") as f:
            data = json.load(f)

        logger.info(f"âœ… ìºì‹±ëœ ë°ì´í„° {len(data)}ê°œ ë¡œë“œ: {cache_file}")
        return data

    def is_cache_up_to_date(self, existing_products: List[Dict], new_products: List[Dict]) -> bool:
        """
        ê¸°ì¡´ ìºì‹± ë°ì´í„°ì™€ ìƒˆë¡œ ê°€ì ¸ì˜¨ DB ë°ì´í„°ë¥¼ ë¹„êµí•˜ì—¬ ë³€ê²½ ì‚¬í•­ì´ ìˆëŠ”ì§€ í™•ì¸.
        """
        existing_dict = {item['id']: item for item in existing_products}
        new_dict = {item['id']: item for item in new_products}

        # ìƒˆë¡œìš´ IDê°€ ì¶”ê°€ë˜ì—ˆê±°ë‚˜ ê¸°ì¡´ ë°ì´í„°ê°€ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if set(existing_dict.keys()) != set(new_dict.keys()):
            logger.info("ğŸ”„ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì¶”ê°€ë¨. ìºì‹±ì„ ê°±ì‹ í•©ë‹ˆë‹¤.")
            return False

        for key in new_dict.keys():
            if existing_dict[key] != new_dict[key]:  # ë°ì´í„° ë³€ê²½ í™•ì¸
                logger.info("ğŸ”„ ê¸°ì¡´ ë°ì´í„°ê°€ ë³€ê²½ë¨. ìºì‹±ì„ ê°±ì‹ í•©ë‹ˆë‹¤.")
                return False

        return True

    def force_generate_cache(self) -> None:
        """
        ê°•ì œë¡œ JSON ìºì‹± íŒŒì¼ì„ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ.
        """
        logger.info("ê°•ì œ ìºì‹± ìƒì„± ìš”ì²­ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.")
        # self.cache_perfume_data(force=True)
        self.cache_perfume_data()
        self.cache_diffuser_data()
        self.cache_note_data()
        self.cache_spice_data()
        logger.info("âœ… ê°•ì œ ìºì‹± ìƒì„± ì™„ë£Œ.")

    def cache_note_data(self) -> None:
        query = """
        SELECT id, note_type, product_id, spice_id FROM note
        """
        self.cache_data(query, self.cache_path_prefix / "note_cache.json", key_field="id")
    
    def cache_perfume_data(self) -> None:
        query = """
        SELECT p.id, p.name_kr, p.name_en, p.brand, p.main_accord, p.category_id FROM product p WHERE p.category_id = 1
        """
        self.cache_data(query, self.cache_path_prefix / "perfume_cache.json", key_field="id")

    def cache_diffuser_data(self) -> None:
        query = """
        SELECT p.id, p.name_kr, p.name_en, p.brand, p.category_id, p.content FROM product p WHERE p.category_id = 2
        """
        self.cache_data(query, self.cache_path_prefix / "diffuser_cache.json", key_field="id")

    def cache_spice_data(self) -> None:
        query = """
        SELECT id, content_en, content_kr, name_en, name_kr, line_id FROM spice
        """
        self.cache_data(query, self.cache_path_prefix / "spice_cache.json", key_field="id")
    
    def load_cached_note_data(self) -> List[Dict]:
        """
        Load cached note data from note_cache.json.
        """
        return self.load_cached_data(self.cache_path_prefix / "note_cache.json")
    
    def load_cached_perfume_data(self) -> List[Dict]:
        """
        Load cached spice data from perfume_cache.json.
        """
        return self.load_cached_data(self.cache_path_prefix / "perfume_cache.json")
    
    def load_cached_diffuser_data(self) -> List[Dict]:
        """
        Load cached spice data from perfume_cache.json.
        """
        return self.load_cached_data(self.cache_path_prefix / "diffuser_cache.json")

    def load_cached_spice_data(self) -> List[Dict]:
        """
        Load cached spice data from spice_cache.json.
        """
        return self.load_cached_data(self.cache_path_prefix / "spice_cache.json")
    
    def get_product_details(self, product_id, products):
        for product in products:
            if product["id"] == product_id:
                return product
        return None
    
    def generate_scent_description(self, notes_text, diffuser_description):
        prompt = f"""Based on the following fragrance combination of the diffuser, describe the characteristics of the overall scent using common perfumery terms such as ìš°ë””, í”Œë¡œëŸ´, ìŠ¤íŒŒì´ì‹œ, ì‹œíŠ¸ëŸ¬ìŠ¤, í—ˆë¸Œ, ë¨¸ìŠ¤í¬, ì•„ì¿ ì•„, ê·¸ë¦°, êµ¬ë¥´ë§, í‘¸ì œë¥´, ì•Œë°í•˜ì´ë“œ, íŒŒìš°ë”ë¦¬, ìŠ¤ëª¨í‚¤, í”„ë£¨í‹°, ì˜¤ë¦¬ì—”íƒˆ, etc. You do not need to break down each note, just focus on the overall scent impression.
            # EXAMPLE 1:
            - Note: Top: ì´íƒˆë¦¬ì•ˆ ë ˆëª¬ ì, ë¡œì¦ˆë§ˆë¦¬\nMiddle: ììŠ¤ë¯¼, ë¼ë°˜ë”˜\nBase: ì‹œë”ìš°ë“œ, ë¨¸ìŠ¤í¬
            - Diffuser Description: ë‹¹ì‹ ì˜ ì—¬ì •ì— ê°ê°ì ì´ê³  ì‹ ì„ í•œ í–¥ê¸°ê°€ í¼ì§‘ë‹ˆë‹¤. ì•„ì¹¨ í–‡ì‚´ì´ ì°½ë¬¸ì„ í†µí•´ ë“¤ì–´ì˜¬ ë•Œ, ì‚°ë“¤ ë°”ëŒê³¼ í•¨ê»˜ ì´íƒˆë¦¬ì•„ ì‹œê³¨ì„ ì—°ìƒì‹œí‚¤ëŠ” í‘¸ë¥¸ í–¥ê¸°
            - Response: ìƒì¾Œí•œ ì‹œíŠ¸ëŸ¬ìŠ¤ì™€ í—ˆë¸Œì˜ ì¡°í™”, í”Œë¡œëŸ´í•œ ìš°ì•„í•¨, ë”°ëœ»í•œ ìš°ë””í•œ í–¥ê³¼ ë¶€ë“œëŸ¬ìš´ ë¨¸ìŠ¤í¬ê°€ ì–´ìš°ëŸ¬ì ¸ ê· í˜• ì¡íŒ í–¥ê¸°ë¥¼ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤. ì „ì²´ì ìœ¼ë¡œ ì´ í–¥ì€ í™œë ¥ì„ ì£¼ë©´ì„œë„ ë™ì‹œì— í¸ì•ˆí•¨ê³¼ ì•ˆì •ê°ì„ ëŠë‚„ ìˆ˜ ìˆëŠ”, ë‹¤ì±„ë¡­ê³  ë§¤ë ¥ì ì¸ í–¥ì…ë‹ˆë‹¤.

            # EXAMPLE 2:
            - Note: single: ì´íƒˆë¦¬ì•ˆ ë² ë¥´ê°€ëª», ì´íƒˆë¦¬ì•ˆ ë ˆëª¬, ìëª½, ë¬´í™”ê³¼, í•‘í¬ í˜í¼, ììŠ¤ë¯¼ ê½ƒì, ë¬´í™”ê³¼ ë‚˜ë¬´, ì‹œë”ìš°ë“œ, ë²¤ì¡°ì¸
            - Diffuser Description: ë‹¹ì‹ ì˜ ì—¬ì •ì— ê°ê°ì ì´ê³  ì‹ ì„ í•œ í–¥ê¸°ê°€ í¼ì§‘ë‹ˆë‹¤. ì•„ì¹¨ í–‡ì‚´ì´ ì°½ë¬¸ì„ í†µí•´ ë“¤ì–´ì˜¬ ë•Œ, ì‚°ë“¤ ë°”ëŒê³¼ í•¨ê»˜ ì´íƒˆë¦¬ì•„ ì‹œê³¨ì„ ì—°ìƒì‹œí‚¤ëŠ” í‘¸ë¥¸ í–¥ê¸°
            - Response: ì´ í–¥ì€ ìƒì¾Œí•˜ê³  í™œê¸°ì°¬ ëŠë‚Œì„ ì£¼ë©´ì„œë„, ë¶€ë“œëŸ½ê³  ë”°ëœ»í•œ ê¹Šì´ë¥¼ ì§€ë‹Œ ê· í˜• ì¡íŒ í–¥ì…ë‹ˆë‹¤. ë°ê³  í†¡í†¡ íŠ€ëŠ” ì‹œíŠ¸ëŸ¬ìŠ¤ í–¥ì´ ê¸°ë¶„ì„ ìƒì¾Œí•˜ê²Œ í•´ì£¼ê³ , ë‹¬ì½¤í•˜ê³  ìš°ì•„í•œ í”Œë¡œëŸ´ê³¼ ìì—°ì ì¸ ìš°ë””í•œ ëŠë‚Œì´ ì¡°í™”ë¥¼ ì´ë£¨ë©° ì„¸ë ¨ëœ ë¶„ìœ„ê¸°ë¥¼ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤. ì „ì²´ì ìœ¼ë¡œ ì‹ ì„ í•˜ê³  ì„¸ë ¨ë˜ë©°, ë”°ëœ»í•˜ë©´ì„œë„ í¸ì•ˆí•œ ëŠë‚Œì„ ì£¼ëŠ” ë³µí•©ì ì¸ í–¥ì…ë‹ˆë‹¤.

            # Note: {notes_text}
            # Diffuser Description: {diffuser_description}
            # Response: """
        
        response = self.gpt_client.invoke(prompt).content.strip()

        return response

    # Load or initialize the diffuser scent cache
    def load_diffuser_scent_cache(self):
        """Load diffuser scent descriptions."""
        try:
            with open(self.cache_path_prefix / "diffuser_scent_cache.json", "r", encoding="utf-8") as f:
                return {item["id"]: item["scent_description"] for item in json.load(f)}
        except (FileNotFoundError, json.JSONDecodeError) as e:
            logger.error(f"Error loading diffuser scent data: {e}")
            return {}
    
    def format_notes(self, note_data):
        if "SINGLE" in note_data:
            single_notes = ", ".join(note_data["SINGLE"])
            return f"single: {single_notes}"
        else:
            formatted = []
            for note_type in ["TOP", "MIDDLE", "BASE"]:
                if note_data.get(note_type):
                    notes_str = ", ".join(note_data[note_type])
                    formatted.append(f"{note_type.lower()}: {notes_str}")
            return "\n".join(formatted)

    def save_scent_cache(self, scent_cache):
        # Update scent cache to a list before saving
        scent_cache_list = [{"id": int(product_id), "scent_description": scent_description} 
                            for product_id, scent_description in scent_cache.items()]
        with open("cache/diffuser_scent_cache.json", "w", encoding="utf-8") as f:
            json.dump(scent_cache_list, f, ensure_ascii=False, indent=4)

    def save_diffuser_scent_description(self) -> None:
        notes = self.load_cached_note_data()
        spices = self.load_cached_spice_data()
        products = self.load_cached_diffuser_data()

        # Extract product IDs from the product cache
        existing_product_ids = {product["id"] for product in products}

        # Create spice ID to name mapping
        spice_id_to_name = {spice["id"]: spice["name_kr"] for spice in spices}

        # Group notes by product_id
        product_notes = defaultdict(lambda: defaultdict(list))

        note_types = ["TOP", "MIDDLE", "BASE", "SINGLE"]

        for note in notes:
            note_type = note["note_type"].upper()
            product_id = note["product_id"]
            if product_id in existing_product_ids:
                spice_name = spice_id_to_name.get(note["spice_id"], "")
                if note_type in note_types and spice_name:
                    product_notes[product_id][note_type].append(spice_name)
        
        # Load the scent cache as a dictionary
        scent_cache = self.load_diffuser_scent_cache()

        # Generate and cache scent descriptions
        scent_cache_list = []

        for product_id, note_data in product_notes.items():
            if str(product_id) in scent_cache:
                logger.info(f"Product {product_id} already has a cached scent description.")
                scent_cache_list.append({
                    "id": int(product_id),
                    "scent_description": scent_cache[str(product_id)]
                })
                continue

            formatted_notes = self.format_notes(note_data)
            logger.info(f"Generating scent description for product {product_id}...")

            product_details = self.get_product_details(product_id, products)
            if product_details:
                # Diffuser description is fetched from product details or assigned manually
                diffuser_description = product_details.get("content", "")

            scent_description = self.generate_scent_description(formatted_notes, diffuser_description)
            scent_cache[str(product_id)] = scent_description

            logger.info(f"Scent description for product {product_id}: {scent_description}")

        # Save the updated scent cache as a list
        self.save_scent_cache(scent_cache)

        logger.info("All scent descriptions have been updated and saved.")

    def get_spices_by_names(self, note_names: List[str]) -> List[Dict]:
        """í–¥ë£Œ ì´ë¦„ìœ¼ë¡œ IDë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            # LIKE ê²€ìƒ‰ì„ ìœ„í•œ íŒ¨í„´ ìƒì„±
            patterns = [f"name_kr LIKE '%{note.strip()}%'" for note in note_names] # í•œê¸€ ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰
            where_clause = " OR ".join(patterns) # OR ì¡°ê±´ìœ¼ë¡œ ì—°ê²°
            
            query = f"""
                SELECT id, name_kr
                FROM spice 
                WHERE {where_clause}
                ORDER BY 
                    CASE 
                        WHEN name_kr IN ({', '.join([f"'{note.strip()}'" for note in note_names])}) THEN 0 
                        ELSE 1 
                    END,
                    name_kr;
            """
            
            with self.connection.cursor() as cursor:
                cursor.execute(query) # ì¿¼ë¦¬ ì‹¤í–‰
                result = cursor.fetchall() # ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
                
                logger.info(f"âœ… ìš”ì²­ëœ í–¥ë£Œ: {note_names}")
                logger.info(f"âœ… ë§¤ì¹­ëœ í–¥ë£Œ: {[r['name_kr'] for r in result]}")
                
                return result
                
        except pymysql.MySQLError as e:
            logger.error(f"ğŸš¨ í–¥ë£Œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    def get_diffusers_by_spice_ids(self, spice_ids: List[int]) -> List[Dict]:
        """í•´ë‹¹ í–¥ë£Œê°€ í•˜ë‚˜ë¼ë„ í¬í•¨ëœ ë””í“¨ì €ë“¤ ì¤‘ì—ì„œ ëœë¤í•˜ê²Œ 2ê°œë¥¼ ì„ íƒí•©ë‹ˆë‹¤."""
        try:
            spice_ids_str = ",".join(map(str, spice_ids))
            
            # ë¨¼ì € ì „ì²´ ë§¤ì¹­ë˜ëŠ” ë””í“¨ì € ìˆ˜ë¥¼ í™•ì¸
            count_query = f"""
                SELECT COUNT(DISTINCT p.id) as total_count
                FROM product p
                JOIN note n ON p.id = n.product_id
                WHERE p.category_id = 2
                AND n.spice_id IN ({spice_ids_str})
                AND p.name_kr NOT LIKE '%ì¹´ ë””í“¨ì €%'
            """
            
            # ê·¸ ë‹¤ìŒ ëœë¤í•˜ê²Œ 2ê°œ ì„ íƒ
            main_query = f"""
                SELECT DISTINCT
                    p.id, 
                    p.brand, 
                    p.name_kr, 
                    p.size_option as volume,
                    p.content,
                    COUNT(DISTINCT n.spice_id) as matching_count,
                    GROUP_CONCAT(DISTINCT s.name_kr) as included_notes
                FROM product p
                JOIN note n ON p.id = n.product_id
                JOIN spice s ON n.spice_id = s.id
                WHERE p.category_id = 2
                AND n.spice_id IN ({spice_ids_str})
                AND p.name_kr NOT LIKE '%ì¹´ ë””í“¨ì €%'
                GROUP BY p.id, p.brand, p.name_kr, p.size_option, p.content
                ORDER BY RAND()
                LIMIT 2
            """
            
            with self.connection.cursor() as cursor:
                # ì „ì²´ ê°œìˆ˜ í™•ì¸
                cursor.execute(count_query)
                total_count = cursor.fetchone()['total_count']
                logger.info(f"âœ… ì „ì²´ ë§¤ì¹­ë˜ëŠ” ë””í“¨ì €: {total_count}ê°œ")
                
                # ëœë¤ ì„ íƒ
                cursor.execute(main_query)
                result = cursor.fetchall()
                
                # ì„ íƒëœ ë””í“¨ì € ë¡œê¹…
                for diffuser in result:
                    logger.info(
                        f"âœ… ì„ íƒë¨: {diffuser['name_kr']} (ID: {diffuser['id']}) - "
                        f"í¬í•¨ í–¥ë£Œ: {diffuser['included_notes']}"
                    )
                
                return result
                
        except pymysql.MySQLError as e:
            logger.error(f"ğŸš¨ ë””í“¨ì € ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
        
    # ORMì„ ì‚¬ìš©í•˜ëŠ” ìƒˆë¡œìš´ ë©”ì„œë“œë“¤
    def get_product_by_id(self, product_id: int):
        """SQLAlchemyë¥¼ ì‚¬ìš©í•˜ì—¬ ì œí’ˆ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
        try:
            return self.session.query(Product).filter(Product.id == product_id).first()
        except Exception as e:
            logger.error(f"ğŸš¨ ì œí’ˆ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None

    def get_similar_products_by_text(self, product_id: int) -> List[Dict]:
        """í…ìŠ¤íŠ¸ ê¸°ë°˜ ìœ ì‚¬ë„ë¡œ ë¹„ìŠ·í•œ ì œí’ˆì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
        try:
            similar_products = (
                self.session.query(
                    Product.id,
                    Product.brand,
                    Product.name_kr,
                    Product.size_option.label('volume'),
                    SimilarText.similarity_score
                )
                .join(SimilarText, Product.id == SimilarText.similar_product_id)
                .filter(SimilarText.product_id == product_id)
                .order_by(SimilarText.similarity_score.desc())
                .limit(5)
                .all()
            )
            logger.info(f"âœ… í…ìŠ¤íŠ¸ ê¸°ë°˜ ìœ ì‚¬ ì œí’ˆ {len(similar_products)}ê°œ ì¡°íšŒ ì™„ë£Œ")
            return [dict(zip(['id', 'brand', 'name_kr', 'volume', 'similarity_score'], p)) for p in similar_products]
        except Exception as e:
            logger.error(f"ğŸš¨ í…ìŠ¤íŠ¸ ê¸°ë°˜ ìœ ì‚¬ ì œí’ˆ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def query_gpt_for_therapeutic_effect(self, spice_name):
        # spiceë§ˆë‹¤ 6ê°œ ì¹´í…Œê³ ë¦¬(ìŠ¤íŠ¸ë ˆìŠ¤ ê°ì†Œ[1], í–‰ë³µ[2], ë¦¬í”„ë ˆì‹œ[3], ìˆ˜ë©´[4], ì§‘ì¤‘[5], ì—ë„ˆì§€[6]) ì¤‘ ì–´ë–¤ íš¨ëŠ¥ì´ ìˆëŠ”ì§€ ë˜ëŠ” ê´€ë ¨ ì—†ëŠ”ì§€[0] GPTì— í™•ì¸ ìš”ì²­í•˜ì—¬ response ì €ì¥ (íŠ¹ì • ì˜ ì•Œë ¤ì§„ í–¥ë£Œë§Œ ì¶”ì²œë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•¨)
        prompt = f"""
        Given the perfumery spice "{spice_name}", determine its primary effect among the following categories:
        1. Stress Reduction (ìŠ¤íŠ¸ë ˆìŠ¤ ê°ì†Œ)
        2. Happiness (í–‰ë³µ)
        3. Refreshing (ë¦¬í”„ë ˆì‹œ)
        4. Sleep Aid (ìˆ˜ë©´)
        5. Concentration (ì§‘ì¤‘)
        6. Energy Boost (ì—ë„ˆì§€)
        0. Neither
        **If none of these apply, return 0.**
        Respond with only the corresponding number.
        """

        response = self.gpt_client.invoke(prompt).content.strip()

        try:
            return int(response)
        except:
            return 0  # Default to 0 if parsing fails

    def load_json(self, file_path):
        if os.path.exists(file_path):
            with open(file_path, "r", encoding="utf-8") as f:
                return json.load(f)
        return []
    
    def save_json(self, file_path, data):
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)

    def save_spice_therapeutic_effect_cache(self):
        spice_therapeutic_effect_cache_file = self.cache_path_prefix / "spice_therapeutic_effect_cache.json"
        
        spice_data = self.load_cached_spice_data()
        spice_therapeutic_effect_data = self.load_json(spice_therapeutic_effect_cache_file)
        spice_therapeutic_effect_dict = {entry["id"]: entry for entry in spice_therapeutic_effect_data}
        
        updated = False
        for spice in spice_data:
            if spice["id"] not in spice_therapeutic_effect_dict:
                spice_therapeutic_effect_value = self.query_gpt_for_therapeutic_effect(spice["name_en"])
                spice_therapeutic_effect_entry = {"id": spice["id"], "name_en": spice["name_en"], "effect": spice_therapeutic_effect_value}
                spice_therapeutic_effect_data.append(spice_therapeutic_effect_entry)
                spice_therapeutic_effect_dict[spice["name_en"]] = spice_therapeutic_effect_entry
                updated = True
        
        if updated:
            self.save_json(spice_therapeutic_effect_cache_file, spice_therapeutic_effect_data)
            print("spice_therapeutic_effect_cache.json has been updated.")
        else:
            print("All spices already have an entry in spice_therapeutic_effect_cache.json.")

    def load_cached_spice_therapeutic_effect_data(self):
        """Load spice therapeutic effect data from cache."""
        try:
            with open(self.cache_path_prefix / "spice_therapeutic_effect_cache.json", "r", encoding="utf-8") as f:
                return json.load(f)
        except FileNotFoundError:
            logger.error("spice_therapeutic_effect_cache.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        except json.JSONDecodeError:
            logger.error("spice_therapeutic_effect_cache.json íŒŒì¼ì„ íŒŒì‹±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            return []
    
# ìºì‹± ìƒì„± ê¸°ëŠ¥ ì‹¤í–‰
if __name__ == "__main__":
    import os

    # DB ì„¤ì •
    db_config = {
        "host": os.getenv("DB_HOST"),
        "port": os.getenv("DB_PORT"),
        "user": os.getenv("DB_USER"),
        "password": os.getenv("DB_PASSWORD"),
        "database": os.getenv("DB_NAME"),
    }

    # DB ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    db_service = DBService(db_config=db_config)

    # ê°•ì œ ìºì‹± ìƒì„± ì‹¤í–‰
    db_service.force_generate_cache()
    logger.info("í–¥ìˆ˜ ë°ì´í„° ê°•ì œ ìºì‹± ì™„ë£Œ!")
