import os
import torch
import traceback
from PIL import Image, ImageDraw
from transformers import AutoProcessor, AutoModelForCausalLM
import openai  
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from io import BytesIO

# OpenMP ì¶©ëŒ ë°©ì§€ ë° í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

class ImageProcessingService:
    def __init__(self):
        try:
            print("ğŸ”¹ Florence ëª¨ë¸ ë¡œë“œ ì¤‘...")
            # Florence ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ë¡œë“œ
            self.processor = AutoProcessor.from_pretrained(
                "microsoft/Florence-2-large", trust_remote_code=True
            )
            self.model = AutoModelForCausalLM.from_pretrained(
                "microsoft/Florence-2-large", trust_remote_code=True, torch_dtype=torch.float16
            )
            self.device = "cuda:0" if torch.cuda.is_available() else "cpu"
            
            if self.device == "cpu":
                print("ğŸš¨ CUDA ì‚¬ìš© ë¶ˆê°€ëŠ¥! CPUë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            else:
                print("âœ… CUDA ì‚¬ìš© ê°€ëŠ¥, GPUë¡œ ëª¨ë¸ ì‹¤í–‰!")

            self.model.to(self.device)
            print("âœ… Florence ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

            # OpenAI GPT ëª¨ë¸ ì´ˆê¸°í™”
            print("ğŸ”¹ OpenAI GPT ëª¨ë¸ ë¡œë“œ ì¤‘...")
            openai_api_key = os.getenv("OPENAI_API_KEY")  # í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
            if not openai_api_key:
                raise ValueError("ğŸš¨ OpenAI API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

            self.chat = ChatOpenAI(model="gpt-4o", temperature=0.5, api_key=openai_api_key)
            self.root_client = openai.OpenAI(api_key=openai_api_key)  

            self.prompt = ChatPromptTemplate.from_messages([
                ("system", "ë‹¹ì‹ ì€ ì´ë¯¸ì§€ ì„¤ëª…ì„ ê°ì„±ì ì´ê³  ì–´ë–¤ ëŠë‚Œì´ ë“œëŠ”ì§€ ë°”ê¿”ì£¼ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
                ("user", "ë‹¤ìŒ ì´ë¯¸ì§€ ì„¤ëª…ì„ ë” ê°ì„±ì ì´ê³  ì–´ë–¤ ëŠë‚Œì´ ë“¤ê³  ì–´ë–¤ í–¥ì´ ì–´ìš¸ë¦´ì§€ ì¶”ì²œë°›ê³  í–¥ìˆ˜ë¥¼ ì°¾ì•„ë‹¬ë¼ê³ ë¥¼ í•œ ì¤„ë¡œ ë°”ê¿”ì£¼ì„¸ìš”: '{description}'")
            ])
            print("âœ… OpenAI GPT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        except Exception as e:
            error_trace = traceback.format_exc()
            print(f"ğŸš¨ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\n{error_trace}")
            raise RuntimeError(f"ğŸš¨ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    def get_emotional_caption(self, description: str) -> str:
        """ GPTë¥¼ ì´ìš©í•´ ê°ì„±ì ì¸ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± """
        try:
            chain = self.prompt | self.chat
            result = chain.invoke({"description": description})
            return result.content
        except Exception as e:
            error_trace = traceback.format_exc()
            print(f"ğŸš¨ GPT ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\n{error_trace}")
            return description

    def process_image(self, image_data: bytes) -> dict:
        """ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ì—¬ ìº¡ì…˜ì„ ìƒì„±í•˜ê³  ê°ì„±ì ì¸ ì„¤ëª…ìœ¼ë¡œ ë³€í™˜ """
        try:
            print("ğŸ”¹ ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘...")
            image = Image.open(BytesIO(image_data)).convert("RGB")
            image = image.resize((512, 512))

            # Florence ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì„¤ëª… ìƒì„±
            prompt = "<MORE_DETAILED_CAPTION>"
            inputs = self.processor(text=prompt, images=image, return_tensors="pt")

            # âœ… ëª¨ë“  ì…ë ¥ ë°ì´í„°ë¥¼ self.deviceë¡œ ì´ë™ (ì˜¤ë¥˜ ë°©ì§€)
            inputs["input_ids"] = inputs["input_ids"].to(self.device)
            inputs["pixel_values"] = inputs["pixel_values"].to(dtype=torch.float16, device=self.device)

            generated_ids = self.model.generate(
                input_ids=inputs["input_ids"],
                pixel_values=inputs["pixel_values"],
                max_new_tokens=256,  # âœ… ë©”ëª¨ë¦¬ ì´ˆê³¼ ë°©ì§€ë¥¼ ìœ„í•´ í† í° ìˆ˜ ì¤„ì„
                num_beams=5,
                do_sample=True,
                top_k=50,
                temperature=0.7
            )

            # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
            generated_text = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
            print("âœ… Florence ëª¨ë¸ ìƒì„± ê²°ê³¼:", generated_text)

            # GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì„±ì ì¸ ì„¤ëª… ìƒì„±
            description = generated_text
            feeling = self.get_emotional_caption(description)

            return {
                "description": description,
                "feeling": feeling
            }
        except Exception as e:
            error_trace = traceback.format_exc()
            print(f"ğŸš¨ ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\n{error_trace}")
            raise RuntimeError(f"ğŸš¨ ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def draw_boxes(self, image, results):
        """ ì´ë¯¸ì§€ ìœ„ì— ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ê·¸ë¦¬ê³  ë¼ë²¨ì„ í‘œì‹œ """
        draw = ImageDraw.Draw(image)
        bboxes = results.get("<DENSE_REGION_CAPTION>", {}).get("bboxes", [])
        labels = results.get("<DENSE_REGION_CAPTION>", {}).get("labels", [])

        for bbox, label in zip(bboxes, labels):
            x1, y1, x2, y2 = bbox
            draw.rectangle([x1, y1, x2, y2], outline="red", width=3)
            draw.text((x1, y1 - 10), label, fill="red")

        return image